{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anand/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Tuple, List, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"neo4j+s://71d8f74b.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"4ZuShP-Ats28MznYfC9kjCW5nShc6FT6iSho69Z9FVU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
    "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
    "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "\n",
    "# Assuming you have these dependencies installed\n",
    "# If using spaCy (mentioned in extract_entities function):\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Text extraction functions from your original code\n",
    "def extract_text_from_file(file):\n",
    "    \"\"\"\n",
    "    Extract text content from various file formats.\n",
    "    \n",
    "    Args:\n",
    "        file: Uploaded file object\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text\n",
    "    \"\"\"\n",
    "    file_name = file.name.lower()\n",
    "    file_content = file.read()\n",
    "    \n",
    "    # Create a temporary file to store content\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as tmp_file:\n",
    "        tmp_file.write(file_content)\n",
    "        tmp_file_path = tmp_file.name\n",
    "    \n",
    "    try:\n",
    "        # Extract text based on file type\n",
    "        if file_name.endswith('.pdf'):\n",
    "            text = extract_from_pdf(tmp_file_path)\n",
    "        elif file_name.endswith('.docx'):\n",
    "            text = extract_from_docx(tmp_file_path)\n",
    "        elif file_name.endswith('.txt'):\n",
    "            try:\n",
    "                with open(tmp_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                # Fallback for text files\n",
    "                text = file_content.decode('utf-8', errors='ignore')\n",
    "        else:\n",
    "            text = \"\"\n",
    "            raise ValueError(f\"Unsupported file format: {os.path.splitext(file_name)[1]}\")\n",
    "    except Exception as e:\n",
    "        # Log the error for debugging\n",
    "        print(f\"Error extracting text: {str(e)}\")\n",
    "        # Provide a generic error message\n",
    "        text = f\"Error processing file: {str(e)}\"\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if os.path.exists(tmp_file_path):\n",
    "            os.unlink(tmp_file_path)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_from_pdf(file_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        text = f\"Error extracting PDF content: {str(e)}\"\n",
    "    return text\n",
    "\n",
    "def extract_from_docx(file_path):\n",
    "    \"\"\"Extract text from DOCX file\"\"\"\n",
    "    try:\n",
    "        text = docx2txt.process(file_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting DOCX content: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_chunks(text, window_size=50000, overlap=10000):\n",
    "    \"\"\"\n",
    "    Split large text into overlapping chunks using a sliding window approach.\n",
    "    Attempts to break at natural boundaries like paragraphs or sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        window_size: Maximum size of each chunk in characters\n",
    "        overlap: Overlap between consecutive chunks in characters\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples containing (start_pos, end_pos, chunk_text)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Only chunk if text is larger than window size\n",
    "    if len(text) <= window_size:\n",
    "        return [(0, len(text), text)]\n",
    "    \n",
    "    # Create overlapping chunks with sliding window approach\n",
    "    start_positions = range(0, len(text), window_size - overlap)\n",
    "    \n",
    "    for pos in start_positions:\n",
    "        # Calculate the ideal end position\n",
    "        ideal_end = min(pos + window_size, len(text))\n",
    "        \n",
    "        # If not at the end of text, try to find a natural boundary near the ideal end\n",
    "        if ideal_end < len(text):\n",
    "            # Look for paragraph break near the ideal end\n",
    "            paragraph_break = text.rfind('\\n\\n', pos, ideal_end)\n",
    "            if paragraph_break != -1 and paragraph_break > pos + (window_size // 2):\n",
    "                # If paragraph break found and it's not too close to the start, use it\n",
    "                end_pos = paragraph_break\n",
    "            else:\n",
    "                # Look for sentence break (period followed by space)\n",
    "                sentence_break = text.rfind('. ', pos + (window_size // 2), ideal_end)\n",
    "                if sentence_break != -1:\n",
    "                    end_pos = sentence_break + 1  # Include the period\n",
    "                else:\n",
    "                    # No natural break found, use the ideal end\n",
    "                    end_pos = ideal_end\n",
    "        else:\n",
    "            # At the end of text, use the remaining text\n",
    "            end_pos = ideal_end\n",
    "            \n",
    "        chunks.append((pos, end_pos, text[pos:end_pos]))\n",
    "        \n",
    "        # Break if we've reached the end of the text\n",
    "        if end_pos >= len(text):\n",
    "            break\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Implementation using LangChain\n",
    "class PDFRagSystem:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\", temperature=0, chunk_size=1000, chunk_overlap=200):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system for PDF documents.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the LLM model to use\n",
    "            temperature: Temperature setting for the LLM\n",
    "            chunk_size: Size of text chunks for the document splitter\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.qa_chain = None\n",
    "    \n",
    "    def process_file(self, file):\n",
    "        \"\"\"\n",
    "        Process a PDF file and create a vector database for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            file: File object for a PDF document\n",
    "        \n",
    "        Returns:\n",
    "            self: For method chaining\n",
    "        \"\"\"\n",
    "        # Extract text from the file\n",
    "        print(f\"Processing file: {file.name}\")\n",
    "        text = extract_text_from_file(file)\n",
    "        \n",
    "        # Create LangChain documents from the extracted text\n",
    "        raw_documents = [Document(page_content=text, metadata={\"source\": file.name})]\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        print(\"Splitting document into chunks...\")\n",
    "        documents = self.text_splitter.split_documents(raw_documents)\n",
    "        print(f\"Created {len(documents)} document chunks\")\n",
    "        \n",
    "        # Create vector store\n",
    "        print(\"Creating vector store...\")\n",
    "        self.vectorstore = Chroma.from_documents(documents, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "        \n",
    "        # Create QA chain\n",
    "        template = \"\"\"\n",
    "        You are an AI assistant that helps users answer questions based on the provided document content.\n",
    "        Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        PROMPT = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": PROMPT}\n",
    "        )\n",
    "        \n",
    "        print(\"RAG system ready for queries!\")\n",
    "        return self\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"\n",
    "        Ask a question to the RAG system.\n",
    "        \n",
    "        Args:\n",
    "            question: Question to ask about the document\n",
    "        \n",
    "        Returns:\n",
    "            dict: Answer and source documents\n",
    "        \"\"\"\n",
    "        if not self.qa_chain:\n",
    "            raise ValueError(\"No document has been processed. Call process_file() first.\")\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        # Format and return the result\n",
    "        answer = result[\"result\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "        \n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"\\nSources:\")\n",
    "        for i, doc in enumerate(source_docs):\n",
    "            print(f\"Source {i+1}: {doc.page_content[:150]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"source_documents\": source_docs\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Read the file content\n",
    "with open('Financial Crisis Enquiry Report 2008.pdf', 'rb') as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "# Create a BytesIO object with the content\n",
    "file_obj = io.BytesIO(file_content)\n",
    "file_obj.name = 'Financial Crisis Enquiry Report 2008.pdf'  # This is now allowed\n",
    "\n",
    "# Now use it with your RAG system\n",
    "rag = PDFRagSystem()\n",
    "rag.process_file(file_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag.ask(\"Please identify the detault event. For the event identify the defaulting organization and the time when the default event occurred\")[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinDKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
